# gpu_project
In this project, we propose an approach to parallelize a convolutional layer of a neural network on GPUs using CUDA. Given an input tensor of shape $H_i \times W_i \times C_i$, we apply $N$ convolutional filters, each of size $K \times K \times C_i$, with a stride $S$. This operation produces an output tensor of size $N \times \left( \frac{{H_i - K}}{{S}} + 1 \right) \times \left( \frac{{W_i - K}}{{S}} + 1 \right)$. We evaluate our approach on the first layer of a CNN developed in a previous course. For benchmarking, we compare the execution times of our best CUDA implementation with the original Python implementation running on both CPU and GPU, where GPU execution is facilitated by PyTorch. The faster version of our implementation is 0.354 ms faster than the original one.
